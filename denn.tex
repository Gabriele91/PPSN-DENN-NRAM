\section{Differential Evolution Neural Networks}
We already present an algorithm that optimizes artificial neural networks using Differential Evolution in [REF TO MODE]. 
The evolutionary algorithm is applied according the conventional neuroevolution approach, i.e. to evolve the network weights instead of backpropagation or other optimization methods based on backpropagation. A batch system, similar to that one used in stochastic
gradient descent, is adopted to reduce the computation time.

\subsection{Differential Evolution}
Differential evolution (DE) is a metaheuristics that solves an optimization of a given fitness function $f$ by iteratively improving a population of $NP$ candidate numerical solutions with dimension $D$.
The population evolution proceeds for a certain number of generations or terminates after a given criterion is met.

The initial population can be generated with some strategies, the most used approach is to randomly generate each vector.
In each generation, for every population element, a new vector is generated by means of a mutation and a crossover operators.
Then, a selection operator is used to choose the vectors in the population for the next generation. 

The fist operator used in DE is the {\it differential mutation}. For each vector $x_i$ in the current generation, called {\it target vector}, a vector $\bar y_i$, called {\it donor vector}, is obtained as linear combination of some vectors in the population selected according
to a given strategy.
There exist many variants of the mutation operator (see for instance \cite{surveyDE_2011,surveyDE_2016}). 
The common mutation (called DE/rand/1) is defined as follows:
$$\bar y_i = x_a + F(x_b - x_c)$$

where a, b, c are mutually exclusive indexes.
The crossover operator creates a new vector $y_i$, called {\it trial vector}, by recombining the donor with the corresponding target vector by means of a given procedure. The crossover operator used in this paper is the binomial crossover regulated by a real parameter $CR$.

Finally, the usual selection operator compares each trial vector $y_i$ with the corresponding target vector $x_i$ and keeps the better of them in the population of the next generation.

\begin{figure}
\begin{tikzpicture}[scale=1, node distance = 2.5cm, auto]
    \node [block] (init) {initialize model};    
    \node [block, right of=init, node distance=2.5cm] (mutation) {mutation};
    \node [block, right of=mutation, node distance=2.5cm] (crossover) {crossover};
    \node [decision, right of=crossover, node distance=2.5cm] (evaluation) {evaluation};
    \node [round, right of=evaluation, node distance=2.5cm] (stop) {stop};    
    
    \node [voidpoint, below of=evaluation, node distance=1.5cm] (void1) {};
    \node [voidpoint, below of=mutation, node distance=1.5cm] (void2) {};
    
    \path [arrow] (init) -- (mutation);
    \path [arrow] (mutation) -- (crossover);
    \path [arrow] (crossover) -- (evaluation);
    
    \draw  (evaluation) -- node [near start] {yes} (void1);
    \draw  (void1) -- (void2);
    \path [arrow] (void2) -- (mutation);
    
    \path [arrow] (evaluation) -- node [near start] {no} (stop);
\end{tikzpicture}
\caption{The evolution of a individual.}
\label{fig:genomalife}
\end{figure}

\subsection{DENN}
Since the DE works with continuous values, we can use  
a straightforward representation based on a one-to-one mapping between the weights of the neural network and 
individuals in DE population.

In details, suppose we have a feed-forward neural network with $k$ levels, numbered from $0$ to $k-1$.
Each network level $l$ is defined by a  real valued matrix ${\bf W}^{(l)}$ representing the connection weights and by the bias vector ${\bf b}^{(l)}$. 

Then, each population element $x_i$ is described by a sequence 
$$\langle ({\bf \hat W}^{(i,0)},{\bf b}^{(i,0)}),\dots,({\bf \hat W}^{(i,k-1)},{\bf b}^{(i,k-1)})\rangle,$$
where ${\bf \hat W}^{(i,l)}$ is the vector obtained by linearization of the matrix ${\bf W}^{(i,l)}$, for $l=0,\dots,k-1$.
%The encoding is depicted in Figure 1,
%%\ref{individual}, 
%where the direct mapping is shown.
For a given population element $x_i$, we denote by $x_i^{(h)}$ its $h$--th component, for $h=0,\dots,2k-1$, i.e.
$x_i^{(h)}={\bf \hat W}^{(i,h/2)}$, if $h$ is even, while $x_i^{(h)}={\bf b}^{(i,(h-1)/2)}$ if $h$ is odd.
Note that each component $x_i^{(h)}$ of a solution $x_i$ is a vector whose size depends on the number of neurons of
the associated levels.

*********
TODO
*********
