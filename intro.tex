\section{Introduction}

Recently a research trend of learning algorithms \cite{Graves2014NeuralTM,greve2016evolving,Joulin2015InferringAP,NRAM:2016,zaremba:2016,zaremba:2015} by means deep learning techniques has started. The basic idea is to learn regularities in sequences of symbols generated by simple algorithms which can only be learned by models having the capacity to count and memorize.
Most of these are different implementations of the controller-interface abstraction: they use a neural controller as a ``processor'' and provide different interfaces for input, output and memory. In order to make these models trainable with gradient descent, sometimes the authors made them ``artificiously'' fully differentiable modifying the discrete nature of their interfaces \cite{Graves2014NeuralTM,NRAM:2016}.{\bf verificare}

In this trend, we consider of particular interest the Neural Random-Access Machines, called NRAM and proposed in \cite{NRAM:2016},
because this model is able to solve problems with pointers:
it can learn to solve problems
that require explicit manipulation and dereferencing of pointers and can learn to solve a number of algorithmic problems. Moreover the authors showed that the solutions can generalize well
to inputs longer than ones seen during the training. In particular, for some problems they generalize
to inputs of arbitrary length.

However, as the authors themselves said, the optimization problem resulting from the backpropagating through the
execution trace of the program is very challenging for standard optimization techniques.  

Moreover, aspects adding complexity to the optimization problem are the continuity condition of the optimization function and the fuzzyfication process. For example, since the output are represented as probability distribution, the output of modules is a computationally costly operation. {\bf verificare}

Since this additional cost is due to the use of gradient descent we think that different optimization methods not requiring differentiability can be of help. 

This work is part of a wider project aiming to rewrite a NRAM model totally based on evolutionary optimization. 
In particular, in this paper we present a version of the Neural Random-Access Machines, where the core neural controller is trained with Differential Evolution meta-heuristic instead of the usual backpropagation algorithm.
In particular we propose to use the DENN method already proposed in \cite{denn2017mod} to evolve neural networks by means of Differential Evolution.

The experiments are conducted to evaluate the system from two different points of view: the ability of DENN technique to optimize the network and compete with backpropagation, and the generalization capability of our solutions. In the experiments several algorithmic problems whose solutions required
pointer manipulation and chasing are chosen.
 
This work puts the Differential Evolution meta-heuristic to another test, where the neural networks are deep and with a lot of parameters. {\bf mettere qualche dettaglio sui numeri delle reti}
Even if DENN method has been applied in its simplest form flattening the network deep and a lot of work has to be done, these first results are very encouraging and prove that other efforts should be undertaken. 

The paper is organized as follows: Section \ref{nram} and \ref{denn} will recall the two basic components combined in this work, the NRAM model and DENN technique; then the experimental part is described in Section \ref{experiments} where the main results are also shown; the paper ends with Section \ref{conclusions} where some ideas for future works are depicted and discussed.