\section{Introduction}

Recently a research trend of learning algorithms \cite{Graves2014NeuralTM,Joulin2015InferringAP,others} has started. The basic idea is to learn regularities in sequences of symbols generated by simple algorithms which can only be learned by models having the capacity to count and to memorize.
Most of these are different implementations of the controller-interface abstraction: they use a neural controller as a ``processor'' and provide different interfaces for input, output and memory. In order to make these models trainable with gradient descent, sometimes the authors made them ``artificiously'' fully differentiable modifying the continuous nature of their interfaces \cite{Graves2014NeuralTM,NRAM:2016}.

In this paper we present a different version of the Neural Random-Access Machines, called NRAM and proposed in \cite{NRAM:2016}, where the core neural controller is trained with Differential Evolution meta-heuristic instead of the usual backpropagation algorithm.

The NRAM model is very interesting because it is the first able to solve problems with pointers:
it can learn to solve problems
that require explicit manipulation and dereferencing of pointers and can learn to solve a number of algorithmic problems. Moreover the authors showed that the solutions can generalize well
to inputs longer than ones seen during the training. In particular, for some problems they generalize
to inputs of arbitrary length.

However, as the authors themselves said, the optimization problem resulting from the backpropagating through the
execution trace of the program is very challenging for standard optimization techniques.  

One of the aspect adding complexity to the optimization problem is the continuity demand of optimization function and the fuzzyfication process{\bf verificare}


*************************************

It seems
likely that a method that can search in an easier “abstract” space would be more effective at solving
such problems.

***** parte inserita anche nella NRAM section *********



In particular the NRAM model can learn to solve problems that require explicit manipulation and dereferencing of pointers. 
 The controller, the core part of the NRAM model can be a feedforward neural network or an LSTM, and it is the only
trainable part of the model.

****** pezzi che userei qua e l\`a ****************

 Our work puts the differential evolution meta-heuristic to another test, where the neural networks are deep and with a lot of parameters.

